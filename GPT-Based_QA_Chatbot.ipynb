{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_community pypdf sentence_transformers tiktoken tokenizers faiss-cpu unstructured numpy==1.24.4 nltk==3.9.1 transformers torch tqdm\n",
        "!pip install -q google-colab  # For Colab-specific utilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ925J9gVgMX",
        "outputId": "e20e17a5-d1c9-4991-c6eb-2f18e3006b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from typing import List, Dict\n",
        "import textwrap\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "1Tqsb_THU4Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"  # Open model\n",
        "#MAX_LENGTH = 512"
      ],
      "metadata": {
        "id": "X1-X5K8FWoNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_data_from_urls(urls: List[str]) -> List[Dict]:\n",
        "    \"\"\"Load data from given URLs using LangChain's UnstructuredURLLoader.\"\"\"\n",
        "    loader = UnstructuredURLLoader(urls=urls)\n",
        "    return loader.load()\n",
        "\n",
        "def split_text(data: List[Dict]) -> List[str]:\n",
        "    \"\"\"Split the text data into chunks using LangChain's CharacterTextSplitter.\"\"\"\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator='\\n',\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP\n",
        "    )\n",
        "    return text_splitter.split_documents(data)\n",
        "\n",
        "def create_embeddings():\n",
        "    \"\"\"Create embeddings using HuggingFace with LangChain.\"\"\"\n",
        "    return HuggingFaceEmbeddings()\n",
        "\n",
        "def create_vector_store(text_chunks: List[str], embeddings):\n",
        "    \"\"\"Create a vector store from text chunks and embeddings.\"\"\"\n",
        "    return FAISS.from_documents(text_chunks, embeddings)\n",
        "\n",
        "def create_llm():\n",
        "    \"\"\"Create a GPT-Neo language model using Hugging Face Transformers.\"\"\"\n",
        "    model_name = \"EleutherAI/gpt-neo-1.3B\"  # Open model\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Set truncation to true\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=100,  # New tokens to generate\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "def create_qa_chain(llm, vector_store):\n",
        "    \"\"\"Create a question-answering chain.\"\"\"\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_store.as_retriever()\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    urls = [\n",
        "        'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',\n",
        "        'https://www.mosaicml.com/blog/mpt-7b',\n",
        "        'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',\n",
        "        'https://lmsys.org/blog/2023-03-30-vicuna/'\n",
        "    ]\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    data = load_data_from_urls(urls)\n",
        "    print(\"Splitting text...\")\n",
        "    text_chunks = split_text(data)\n",
        "    print(\"Creating embeddings...\")\n",
        "    embeddings = create_embeddings()\n",
        "    print(\"Creating vector store...\")\n",
        "    vector_store = create_vector_store(text_chunks, embeddings)\n",
        "    print(\"Creating LLM...\")\n",
        "    llm = create_llm()\n",
        "    print(\"Creating QA chain...\")\n",
        "    qa_chain = create_qa_chain(llm, vector_store)\n",
        "\n",
        "    print(\"\\nChatbot is ready! Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        query = input(\"\\nPrompt: \")\n",
        "        if query.lower() == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        result = qa_chain({'query': query})\n",
        "        wrapped_answer = textwrap.fill(result['result'], width=100)\n",
        "        print(f\"\\nAnswer: {wrapped_answer}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ptzkpRmlU0r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD-KTXoPU2pH",
        "outputId": "1f46fdfe-05eb-4087-c7b3-6bc8631d784d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Splitting text...\n",
            "Creating embeddings...\n",
            "Creating vector store...\n",
            "Creating LLM...\n",
            "Creating QA chain...\n",
            "\n",
            "Chatbot is ready! Type 'exit' to quit.\n",
            "\n",
            "Prompt: what do you know about llama2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer,\n",
            "just say that you don't know, don't try to make up an answer.  Pretraining Data The authors utilized\n",
            "a novel mix of data from publicly accessible sources to train the Llama 2 models, excluding any data\n",
            "from Meta’s products or services. They made efforts to erase data from certain sites known for\n",
            "harboring large amounts of personal information about private individuals. They trained the models\n",
            "on 2 trillion tokens of data, believing this amount provided a beneficial performance-cost balance.\n",
            "They also up-sampled the most factual sources to boost knowledge and reduce instances of false\n",
            "information generation or “hallucinations”. Llama 2 Pretrained Model Evaluation Llama 2 models\n",
            "significantly outperform their Llama 1 counterparts: The 70 billion-parameter Llama 2 model notably\n",
            "improves results on the MMLU and BBH benchmarks by roughly 5 and 8 points, respectively, when\n",
            "compared to the 65 billion-parameter Llama 1 model. Llama 2 models with 7 billion and 30 billion\n",
            "parameters outdo MPT models of similar size in all categories except code benchmarks.  RedPajama We\n",
            "included a number of subsets of the RedPajama dataset, which is Together's attempt to replicate\n",
            "LLaMA's training data. Specifically, we used the CommonCrawl, arXiv, Wikipedia, Books, and\n",
            "StackExchange subsets. The Stack We wanted our model to be capable of code generation, so we turned\n",
            "to The Stack, a 6.4TB corpus of code data. We used The Stack Dedup, a variant of the stack that has\n",
            "been approximately deduplicated (via MinHashLSH) to 2.9TB. We selected a subset of 18 of The Stack's\n",
            "358 programming languages in order to reduce dataset size and increase relevance: C C-Sharp C++\n",
            "Common Lisp F-Sharp Fortran Go Haskell Java Ocaml Perl Python Ruby Rust Scala Scheme Shell Tex We\n",
            "chose to have code constitute 10% of the pretraining tokens, as internal experiments showed that we\n",
            "could train on up to 20% code (and 80% natural language) with no negative impact on natural language\n",
            "evaluation.  Supportive. We build models to support our users, not replace them. We are focused on\n",
            "efficient, specialized, and practical AI performance – not a quest for god-like intelligence. We\n",
            "develop tools that help everyday people and everyday firms use AI to unlock creativity, boost their\n",
            "productivity, and open up new economic opportunities. The models are now available in our GitHub\n",
            "repository. We will publish a full technical report in the near future and look forward to ongoing\n",
            "collaboration with developers and researchers as we roll out the Stable LM suite. In addition, we\n",
            "will be kicking off our crowd-sourced RLHF program and working with community efforts such as Open\n",
            "Assistant to create an open source dataset for AI assistants. We will be releasing more models soon\n",
            "and are growing our team. If you are passionate about democratizing access to this technology and\n",
            "experienced in LLMs, please apply here! Guest User Previous Previous Stability AI releases its Image\n",
            "Upscaling API Next Next  Llama 2 models with 7 billion and 30 billion parameters outdo MPT models of\n",
            "similar size in all categories except code benchmarks. In comparison with Falcon models, Llama 2’s 7\n",
            "billion and 34 billion parameter models outperform the 7 billion and 40 billion parameter Falcon\n",
            "models in all benchmark categories. Moreover, the Llama 2 70B model surpasses all open-source\n",
            "models. Comparatively, the Llama 2 70B model performs similarly to the closed-source GPT-3.5\n",
            "(OpenAI, 2023) on the MMLU and GSM8K benchmarks but shows a significant deficit on coding\n",
            "benchmarks. It matches or exceeds the performance of PaLM (540 billion parameters) on nearly all\n",
            "benchmarks. However, there remains a substantial performance gap between the Llama 2 70B model and\n",
            "both GPT-4 and PaLM-2-L. Fine-tuning Supervised Fine-Tuning (SFT)  Question: what do you know about\n",
            "llama2 Helpful Answer: I am a developer, and I have used llama2 for a while. I am not a designer,\n",
            "and I don't know how to use it. I am a developer, and I have used llama2 for a while. I am not a\n",
            "designer, and I don't know how to use it.  Question: what do you know about llama2 Helpful Answer: I\n",
            "am a developer, and I have used llama2 for a while. I\n",
            "\n",
            "Prompt: what is stable ml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer,\n",
            "just say that you don't know, don't try to make up an answer.  Stability AI Launches the First of\n",
            "its Stable LM Suite of Language Models Product 19 Apr Written By Guest User Today, Stability AI\n",
            "released a new open source language model, Stable LM. The Alpha version of the model is available in\n",
            "3 billion and 7 billion parameters, with 15 billion to 65 billion parameter models to follow.\n",
            "Developers can freely inspect, use, and adapt our Stable LM base models for commercial or research\n",
            "purposes, subject to the terms of the CC BY-SA-4.0 license. In 2022, Stability AI drove the public\n",
            "release of Stable Diffusion, a revolutionary image model representing a transparent, open, and\n",
            "scalable alternative to proprietary AI. With the launch of the Stable LM suite of models, Stability\n",
            "AI is continuing to make foundational AI technology accessible to all. Our Stable LM models can\n",
            "generate text and code and will power various downstream applications. They demonstrate how small\n",
            "and efficient models can deliver high performance with appropriate training.  Guest User Previous\n",
            "Previous Stability AI releases its Image Upscaling API Next Next Stability AI Partners with Iconic\n",
            "Artist Peter Gabriel to Launch Series of AI Animation Challenges titled #DiffuseTogether  Training\n",
            "Stability As many teams have documented, training LLMs with billions of parameters on hundreds-to-\n",
            "thousands of GPUs is incredibly challenging. Hardware will fail frequently and in creative and\n",
            "unexpected ways. Loss spikes will derail training. Teams must \"babysit\" the training run 24/7 in\n",
            "case of failures and apply manual interventions when things go wrong. Check out the OPT logbook for\n",
            "a candid example of the many perils awaiting anyone training an LLM. At MosaicML, our research and\n",
            "engineering teams have worked tirelessly over the last 6 months to eliminate these issues. As a\n",
            "result, our MPT-7B training logbook (Figure 5) is very boring! We trained MPT-7B on 1 trillion\n",
            "tokens from start to finish with no human intervention. No loss spikes, no mid-stream learning rate\n",
            "changes, no data skipping, automatic handling of dead GPUs, etc.  Supportive. We build models to\n",
            "support our users, not replace them. We are focused on efficient, specialized, and practical AI\n",
            "performance – not a quest for god-like intelligence. We develop tools that help everyday people and\n",
            "everyday firms use AI to unlock creativity, boost their productivity, and open up new economic\n",
            "opportunities. The models are now available in our GitHub repository. We will publish a full\n",
            "technical report in the near future and look forward to ongoing collaboration with developers and\n",
            "researchers as we roll out the Stable LM suite. In addition, we will be kicking off our crowd-\n",
            "sourced RLHF program and working with community efforts such as Open Assistant to create an open\n",
            "source dataset for AI assistants. We will be releasing more models soon and are growing our team. If\n",
            "you are passionate about democratizing access to this technology and experienced in LLMs, please\n",
            "apply here! Guest User Previous Previous Stability AI releases its Image Upscaling API Next Next\n",
            "Question: what is stable ml Helpful Answer: The Stable LM suite of models is a set of language\n",
            "models that are optimized for stability. Stability is measured by the number of times the model is\n",
            "retrained. The Stable LM suite of models is a set of language models that are optimized for\n",
            "stability. Stability is measured by the number of times the model is retrained.  Stability is\n",
            "measured by the number of times the model is retrained. The Stable LM suite of models is a set of\n",
            "language models that are optimized\n"
          ]
        }
      ]
    }
  ]
}